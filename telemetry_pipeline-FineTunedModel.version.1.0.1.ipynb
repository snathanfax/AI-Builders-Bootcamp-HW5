{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410d8e03",
   "metadata": {},
   "source": [
    "# Telemetry Pipeline - Model Fine Tuning Workflow\n",
    "This notebook generates synthetic switch telemetry data, prepares a model for GPT-2 fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_synthetic_switch_telemetry(num_samples=100):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    timestamps = [start_time - timedelta(minutes=5 * i) for i in range(num_samples)]\n",
    "    names = [f\"QSFP-{i%32+1}\" for i in range(num_samples)]\n",
    "\n",
    "    data = {\n",
    "        'timestamp': timestamps,\n",
    "        'name': names,\n",
    "        'temp': np.random.uniform(20, 80, num_samples),\n",
    "        'trans-volt': np.random.uniform(3.2, 3.5, num_samples)\n",
    "    }\n",
    "\n",
    "    for ch in range(1, 5):\n",
    "        data[f'channel_{ch}_in_pwr'] = np.random.uniform(-3, 3, num_samples)\n",
    "        data[f'channel_{ch}_out_pwr'] = np.random.uniform(-3, 3, num_samples)\n",
    "        data[f'channel_{ch}_laser_bias_cur'] = np.random.uniform(5, 10, num_samples)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = df['timestamp'].astype(str)\n",
    "    df.to_csv('synthetic_switch_telemetry.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = generate_synthetic_switch_telemetry(200)\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141129ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def classify_anomaly(row):\n",
    "    anomalies = []\n",
    "    if row['temp'] > 75:\n",
    "        anomalies.append(\"Overheating\")\n",
    "    if row['trans-volt'] < 3.1 or row['trans-volt'] > 3.5:\n",
    "        anomalies.append(\"Voltage Drift\")\n",
    "    for ch in range(1, 5):\n",
    "        if row[f'channel_{ch}_laser_bias_cur'] > 9:\n",
    "            anomalies.append(f\"Channel {ch} Bias Current Spike\")\n",
    "        if row[f'channel_{ch}_out_pwr'] < -3:\n",
    "            anomalies.append(f\"Channel {ch} Power Loss\")\n",
    "    return \"Anomalous - \" + ', '.join(anomalies) if anomalies else \"Normal\"\n",
    "\n",
    "def row_to_prompt(row):\n",
    "    prompt = (\n",
    "        f\"Telemetry Report:\\n\"\n",
    "        f\"- Timestamp: {row['timestamp']}\\n\"\n",
    "        f\"- Module: {row['name']}\\n\"\n",
    "        f\"- Temperature: {row['temp']:.2f}°C\\n\"\n",
    "        f\"- Transceiver Voltage: {row['trans-volt']:.2f}V\\n\"\n",
    "    )\n",
    "    for ch in range(1, 5):\n",
    "        prompt += (\n",
    "            f\"- Channel {ch} Input Power: {row[f'channel_{ch}_in_pwr']:.2f} dBm\\n\"\n",
    "            f\"- Channel {ch} Output Power: {row[f'channel_{ch}_out_pwr']:.2f} dBm\\n\"\n",
    "            f\"- Channel {ch} Laser Bias Current: {row[f'channel_{ch}_laser_bias_cur']:.2f} mA\\n\"\n",
    "        )\n",
    "    prompt += \"\\nIs this normal or anomalous?\"\n",
    "    return prompt\n",
    "\n",
    "with open('train.jsonl', 'w') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        json.dump({\"prompt\": row_to_prompt(row), \"response\": classify_anomaly(row)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"✅ train.jsonl created.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e63565-5723-4695-b305-845f3ad2bc78",
   "metadata": {},
   "source": [
    "# Safe Fine-Tuning and Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da416019-0a19-4797-adb8-ca8077421847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Debug GPU Information ---\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Force clearer error reporting from CUDA\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset = load_dataset('json', data_files={'train': 'train.jsonl'})\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure tokenizer has padding token (GPT-2 does not have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load Model Safely ---\n",
    "# Set dtype explicitly to match intended precision (can be float16 if using fp16 training)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "\n",
    "# Move to GPU after verifying load works\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# --- Tokenization Helper ---\n",
    "def concatenate_prompt_response(examples):\n",
    "    combined = [\n",
    "        f\"prompt: {p}\\nresponse: {r}\" for p, r in zip(examples['prompt'], examples['response'])\n",
    "    ]\n",
    "    return tokenizer(combined, truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(concatenate_prompt_response, batched=True)\n",
    "tokenized_eval = eval_dataset.map(concatenate_prompt_response, batched=True)\n",
    "\n",
    "# --- Data Collator (dynamic padding) ---\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# --- Trainer Setup ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --- Train Model ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Safe Save (CPU-based) ---\n",
    "print(\"✅ Training complete. Saving model to CPU...\")\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "model.save_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "\n",
    "print(\"✅ Model and tokenizer saved safely to 'fine_tuned_gpt2_telemetry'.\")\n",
    "\n",
    "# --- Post-save Reload Test ---\n",
    "print(\"✅ Reloading saved model for sanity check...\")\n",
    "\n",
    "reloaded_model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_gpt2_telemetry\", torch_dtype=torch.float32)\n",
    "reloaded_model = reloaded_model.to(\"cuda\")  # Move back to GPU\n",
    "\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "\n",
    "# Quick inference test to confirm save/load worked\n",
    "test_input = \"prompt: What is knowledge distillation?\\nresponse:\"\n",
    "inputs = reloaded_tokenizer(test_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model(**inputs)\n",
    "\n",
    "print(f\"✅ Reloaded model test passed. Output shape: {outputs.logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b632b-25a6-43db-907f-6dfaea4eb1f4",
   "metadata": {},
   "source": [
    "# Test for Model Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4021f0-e772-42f1-8bf5-9d6d22a22cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: Quadro M1000M\n",
      "CUDA Version: 11.7\n",
      "PyTorch Version: 2.0.0+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully to CPU.\n",
      "✅ Model moved to GPU successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt2_telemetry\", device_map=None)\n",
    "    print(\"✅ Model loaded successfully to CPU.\")\n",
    "\n",
    "    teacher = teacher.to(\"cuda\")\n",
    "    print(\"✅ Model moved to GPU successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during model load/move: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b32f07-f629-442e-bbc7-e27438297302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
