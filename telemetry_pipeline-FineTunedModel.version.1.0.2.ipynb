{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410d8e03",
   "metadata": {},
   "source": [
    "# Telemetry Pipeline - Model Fine Tuning Workflow\n",
    "This notebook generates synthetic switch telemetry data, prepares a model for GPT-2 fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9af64-7288-405f-b4c2-258529424c6f",
   "metadata": {},
   "source": [
    "                    GNU AFFERO GENERAL PUBLIC LICENSE\n",
    "                       Version 3, 19 November 2007\n",
    "\n",
    "Copyright (C) 2024 Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify  \n",
    "it under the terms of the GNU Affero General Public License as  \n",
    "published by the Free Software Foundation, either version 3 of the  \n",
    "License, or (at your option) any later version.  \n",
    "\n",
    "This program is distributed in the hope that it will be useful,  \n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of  \n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the  \n",
    "GNU Affero General Public License for more details.  \n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License  \n",
    "along with this program. If not, see <https://www.gnu.org/licenses/>.  \n",
    "\n",
    "As per AGPLv3, if you modify this software and make it available over a  \n",
    "network, you must provide the source code of your modifications under the  \n",
    "same license.  \n",
    "\n",
    "For inquiries, please contact:  \n",
    "Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc1df50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>temp</th>\n",
       "      <th>trans-volt</th>\n",
       "      <th>channel_1_in_pwr</th>\n",
       "      <th>channel_1_out_pwr</th>\n",
       "      <th>channel_1_laser_bias_cur</th>\n",
       "      <th>channel_2_in_pwr</th>\n",
       "      <th>channel_2_out_pwr</th>\n",
       "      <th>channel_2_laser_bias_cur</th>\n",
       "      <th>channel_3_in_pwr</th>\n",
       "      <th>channel_3_out_pwr</th>\n",
       "      <th>channel_3_laser_bias_cur</th>\n",
       "      <th>channel_4_in_pwr</th>\n",
       "      <th>channel_4_out_pwr</th>\n",
       "      <th>channel_4_laser_bias_cur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-08 00:44:21.737018</td>\n",
       "      <td>QSFP-1</td>\n",
       "      <td>42.472407</td>\n",
       "      <td>3.392609</td>\n",
       "      <td>-2.381257</td>\n",
       "      <td>-1.986390</td>\n",
       "      <td>8.536193</td>\n",
       "      <td>-1.889202</td>\n",
       "      <td>1.549579</td>\n",
       "      <td>5.835210</td>\n",
       "      <td>-1.848796</td>\n",
       "      <td>2.739008</td>\n",
       "      <td>6.308528</td>\n",
       "      <td>2.860802</td>\n",
       "      <td>-0.956375</td>\n",
       "      <td>8.147788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-08 00:39:21.737018</td>\n",
       "      <td>QSFP-2</td>\n",
       "      <td>77.042858</td>\n",
       "      <td>3.225242</td>\n",
       "      <td>2.415317</td>\n",
       "      <td>-1.328458</td>\n",
       "      <td>5.762695</td>\n",
       "      <td>0.251406</td>\n",
       "      <td>-2.852479</td>\n",
       "      <td>5.838096</td>\n",
       "      <td>-1.059771</td>\n",
       "      <td>1.425050</td>\n",
       "      <td>6.234894</td>\n",
       "      <td>-2.335676</td>\n",
       "      <td>-2.569729</td>\n",
       "      <td>5.271660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-08 00:34:21.737018</td>\n",
       "      <td>QSFP-3</td>\n",
       "      <td>63.919637</td>\n",
       "      <td>3.248489</td>\n",
       "      <td>0.031514</td>\n",
       "      <td>-1.937937</td>\n",
       "      <td>7.881442</td>\n",
       "      <td>2.237675</td>\n",
       "      <td>-2.867259</td>\n",
       "      <td>5.183357</td>\n",
       "      <td>-1.640062</td>\n",
       "      <td>-0.880492</td>\n",
       "      <td>9.531273</td>\n",
       "      <td>-0.464709</td>\n",
       "      <td>-0.542227</td>\n",
       "      <td>8.743226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-08 00:29:21.737018</td>\n",
       "      <td>QSFP-4</td>\n",
       "      <td>55.919509</td>\n",
       "      <td>3.469566</td>\n",
       "      <td>1.958745</td>\n",
       "      <td>-2.467785</td>\n",
       "      <td>8.033575</td>\n",
       "      <td>1.393349</td>\n",
       "      <td>-1.058339</td>\n",
       "      <td>8.682010</td>\n",
       "      <td>-0.870022</td>\n",
       "      <td>-1.220787</td>\n",
       "      <td>6.247731</td>\n",
       "      <td>-2.747852</td>\n",
       "      <td>-1.132695</td>\n",
       "      <td>6.587934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-08 00:24:21.737018</td>\n",
       "      <td>QSFP-5</td>\n",
       "      <td>29.361118</td>\n",
       "      <td>3.381929</td>\n",
       "      <td>-1.079702</td>\n",
       "      <td>-2.276185</td>\n",
       "      <td>7.120653</td>\n",
       "      <td>1.839367</td>\n",
       "      <td>-0.068141</td>\n",
       "      <td>8.319023</td>\n",
       "      <td>-2.583457</td>\n",
       "      <td>-0.901781</td>\n",
       "      <td>6.359749</td>\n",
       "      <td>1.439425</td>\n",
       "      <td>1.062724</td>\n",
       "      <td>5.000673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp    name       temp  trans-volt  \\\n",
       "0  2025-03-08 00:44:21.737018  QSFP-1  42.472407    3.392609   \n",
       "1  2025-03-08 00:39:21.737018  QSFP-2  77.042858    3.225242   \n",
       "2  2025-03-08 00:34:21.737018  QSFP-3  63.919637    3.248489   \n",
       "3  2025-03-08 00:29:21.737018  QSFP-4  55.919509    3.469566   \n",
       "4  2025-03-08 00:24:21.737018  QSFP-5  29.361118    3.381929   \n",
       "\n",
       "   channel_1_in_pwr  channel_1_out_pwr  channel_1_laser_bias_cur  \\\n",
       "0         -2.381257          -1.986390                  8.536193   \n",
       "1          2.415317          -1.328458                  5.762695   \n",
       "2          0.031514          -1.937937                  7.881442   \n",
       "3          1.958745          -2.467785                  8.033575   \n",
       "4         -1.079702          -2.276185                  7.120653   \n",
       "\n",
       "   channel_2_in_pwr  channel_2_out_pwr  channel_2_laser_bias_cur  \\\n",
       "0         -1.889202           1.549579                  5.835210   \n",
       "1          0.251406          -2.852479                  5.838096   \n",
       "2          2.237675          -2.867259                  5.183357   \n",
       "3          1.393349          -1.058339                  8.682010   \n",
       "4          1.839367          -0.068141                  8.319023   \n",
       "\n",
       "   channel_3_in_pwr  channel_3_out_pwr  channel_3_laser_bias_cur  \\\n",
       "0         -1.848796           2.739008                  6.308528   \n",
       "1         -1.059771           1.425050                  6.234894   \n",
       "2         -1.640062          -0.880492                  9.531273   \n",
       "3         -0.870022          -1.220787                  6.247731   \n",
       "4         -2.583457          -0.901781                  6.359749   \n",
       "\n",
       "   channel_4_in_pwr  channel_4_out_pwr  channel_4_laser_bias_cur  \n",
       "0          2.860802          -0.956375                  8.147788  \n",
       "1         -2.335676          -2.569729                  5.271660  \n",
       "2         -0.464709          -0.542227                  8.743226  \n",
       "3         -2.747852          -1.132695                  6.587934  \n",
       "4          1.439425           1.062724                  5.000673  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_synthetic_switch_telemetry(num_samples=100):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    timestamps = [start_time - timedelta(minutes=5 * i) for i in range(num_samples)]\n",
    "    names = [f\"QSFP-{i%32+1}\" for i in range(num_samples)]\n",
    "\n",
    "    data = {\n",
    "        'timestamp': timestamps,\n",
    "        'name': names,\n",
    "        'temp': np.random.uniform(20, 80, num_samples),\n",
    "        'trans-volt': np.random.uniform(3.2, 3.5, num_samples)\n",
    "    }\n",
    "\n",
    "    for ch in range(1, 5):\n",
    "        data[f'channel_{ch}_in_pwr'] = np.random.uniform(-3, 3, num_samples)\n",
    "        data[f'channel_{ch}_out_pwr'] = np.random.uniform(-3, 3, num_samples)\n",
    "        data[f'channel_{ch}_laser_bias_cur'] = np.random.uniform(5, 10, num_samples)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = df['timestamp'].astype(str)\n",
    "    df.to_csv('synthetic_switch_telemetry.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = generate_synthetic_switch_telemetry(200)\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141129ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train.jsonl created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def classify_anomaly(row):\n",
    "    anomalies = []\n",
    "    if row['temp'] > 75:\n",
    "        anomalies.append(\"Overheating\")\n",
    "    if row['trans-volt'] < 3.1 or row['trans-volt'] > 3.5:\n",
    "        anomalies.append(\"Voltage Drift\")\n",
    "    for ch in range(1, 5):\n",
    "        if row[f'channel_{ch}_laser_bias_cur'] > 9:\n",
    "            anomalies.append(f\"Channel {ch} Bias Current Spike\")\n",
    "        if row[f'channel_{ch}_out_pwr'] < -3:\n",
    "            anomalies.append(f\"Channel {ch} Power Loss\")\n",
    "    return \"Anomalous - \" + ', '.join(anomalies) if anomalies else \"Normal\"\n",
    "\n",
    "def row_to_prompt(row):\n",
    "    prompt = (\n",
    "        f\"Telemetry Report:\\n\"\n",
    "        f\"- Timestamp: {row['timestamp']}\\n\"\n",
    "        f\"- Module: {row['name']}\\n\"\n",
    "        f\"- Temperature: {row['temp']:.2f}°C\\n\"\n",
    "        f\"- Transceiver Voltage: {row['trans-volt']:.2f}V\\n\"\n",
    "    )\n",
    "    for ch in range(1, 5):\n",
    "        prompt += (\n",
    "            f\"- Channel {ch} Input Power: {row[f'channel_{ch}_in_pwr']:.2f} dBm\\n\"\n",
    "            f\"- Channel {ch} Output Power: {row[f'channel_{ch}_out_pwr']:.2f} dBm\\n\"\n",
    "            f\"- Channel {ch} Laser Bias Current: {row[f'channel_{ch}_laser_bias_cur']:.2f} mA\\n\"\n",
    "        )\n",
    "    prompt += \"\\nIs this normal or anomalous?\"\n",
    "    return prompt\n",
    "\n",
    "with open('train.jsonl', 'w') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        json.dump({\"prompt\": row_to_prompt(row), \"response\": classify_anomaly(row)}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"✅ train.jsonl created.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e63565-5723-4695-b305-845f3ad2bc78",
   "metadata": {},
   "source": [
    "# Safe Fine-Tuning and Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da416019-0a19-4797-adb8-ca8077421847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "CUDA Available: True\n",
      "CUDA Device: Quadro M1000M\n",
      "CUDA Version: 11.7\n",
      "PyTorch Version: 2.0.0+cu117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2c4a7a23734e71a55f0132a27f6909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f98931646245a9a98dd088b05e62cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f36d6a79e124096ab7c5ba34eed711c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaji.nathan\\AppData\\Local\\Temp\\ipykernel_13724\\1311692355.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='238' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/240 12:40 < 00:06, 0.31 it/s, Epoch 2.96/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.524100</td>\n",
       "      <td>0.480681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.461769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Debug GPU Information ---\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Force clearer error reporting from CUDA\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset = load_dataset('json', data_files={'train': 'train.jsonl'})\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure tokenizer has padding token (GPT-2 does not have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load Model Safely ---\n",
    "# Set dtype explicitly to match intended precision (can be float16 if using fp16 training)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "\n",
    "# Move to GPU after verifying load works\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# --- Tokenization Helper ---\n",
    "def concatenate_prompt_response(examples):\n",
    "    combined = [\n",
    "        f\"prompt: {p}\\nresponse: {r}\" for p, r in zip(examples['prompt'], examples['response'])\n",
    "    ]\n",
    "    return tokenizer(combined, truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(concatenate_prompt_response, batched=True)\n",
    "tokenized_eval = eval_dataset.map(concatenate_prompt_response, batched=True)\n",
    "\n",
    "# --- Data Collator (dynamic padding) ---\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# --- Trainer Setup ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --- Train Model ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Safe Save (CPU-based) ---\n",
    "print(\"✅ Training complete. Saving model to CPU...\")\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "model.save_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "\n",
    "print(\"✅ Model and tokenizer saved safely to 'fine_tuned_gpt2_telemetry'.\")\n",
    "\n",
    "# --- Post-save Reload Test ---\n",
    "print(\"✅ Reloading saved model for sanity check...\")\n",
    "\n",
    "reloaded_model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_gpt2_telemetry\", torch_dtype=torch.float32)\n",
    "reloaded_model = reloaded_model.to(\"cuda\")  # Move back to GPU\n",
    "\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "\n",
    "# Quick inference test to confirm save/load worked\n",
    "test_input = \"prompt: What is knowledge distillation?\\nresponse:\"\n",
    "inputs = reloaded_tokenizer(test_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model(**inputs)\n",
    "\n",
    "print(f\"✅ Reloaded model test passed. Output shape: {outputs.logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b632b-25a6-43db-907f-6dfaea4eb1f4",
   "metadata": {},
   "source": [
    "# Test for Model Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4021f0-e772-42f1-8bf5-9d6d22a22cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt2_telemetry\", device_map=None)\n",
    "    print(\"✅ Model loaded successfully to CPU.\")\n",
    "\n",
    "    teacher = teacher.to(\"cuda\")\n",
    "    print(\"✅ Model moved to GPU successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during model load/move: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b32f07-f629-442e-bbc7-e27438297302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
