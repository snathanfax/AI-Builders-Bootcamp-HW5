{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410d8e03",
   "metadata": {},
   "source": [
    "# Telemetry Pipeline \n",
    "## Model Quantization Workflow for running Inference on Edge Switch\n",
    "#### This notebook takes the distilled_pruned_gpt2_telemetry model, performs model compression using quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8178d-adee-4dde-9905-300954edfd52",
   "metadata": {},
   "source": [
    "                    GNU AFFERO GENERAL PUBLIC LICENSE\n",
    "                       Version 3, 19 November 2007\n",
    "\n",
    "Copyright (C) 2024 Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify  \n",
    "it under the terms of the GNU Affero General Public License as  \n",
    "published by the Free Software Foundation, either version 3 of the  \n",
    "License, or (at your option) any later version.  \n",
    "\n",
    "This program is distributed in the hope that it will be useful,  \n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of  \n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the  \n",
    "GNU Affero General Public License for more details.  \n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License  \n",
    "along with this program. If not, see <https://www.gnu.org/licenses/>.  \n",
    "\n",
    "As per AGPLv3, if you modify this software and make it available over a  \n",
    "network, you must provide the source code of your modifications under the  \n",
    "same license.  \n",
    "\n",
    "For inquiries, please contact:  \n",
    "Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e9fca-c7d9-4f86-b776-6ebdb57f288d",
   "metadata": {},
   "source": [
    "## Check Distilled and Pruned Model for corruption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf06804f-35d8-432f-8888-3ddaefa3eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "torchvision.disable_beta_transforms_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64fc25b-dc81-4a80-b481-93450262e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\"./distilled_pruned_gpt2_telemetry\", device_map=None)\n",
    "    print(\"‚úÖ Model loaded successfully to CPU.\")\n",
    "\n",
    "    teacher = teacher.to(\"cuda\")\n",
    "    print(\"‚úÖ Model moved to GPU successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during model load/move: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f02e9-ad69-4b77-a7c3-33a0f26b5677",
   "metadata": {},
   "source": [
    "# Quantization of the model \n",
    "### Load and Quantize  GPT-2 Model in 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f018ec9d-84fe-4bbc-8ab7-25d17b49a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abfcea6c-f65f-4631-9c25-211b255046e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Tokenizer\n",
    "print(\"üîπ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Load the base tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089e1cb9-33e0-4a29-85b1-1389f0864463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed padding issue: Add padding token (GPT-2 does not have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use the end-of-sequence token as padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb55df94-ff75-4d44-b33c-761636b6b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Defining 4-bit quantization configuration...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the 4-bit Quantization Configuration\n",
    "print(\"üîπ Defining 4-bit quantization configuration...\")\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # Use 4-bit quantization\n",
    "    group_size=128,  # Grouped quantization for better efficiency\n",
    "    desc_act=False  # Disable activation-descending quantization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5e75fd-c1e6-4228-bab4-7a5d1f6c2854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./distilled_pruned_gpt2_telemetry were not used when initializing GPT2LMHeadModel: ['lm_head.weight_mask']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load the Pruned and Distilled GPT-2 Model\n",
    "print(\"üîπ Loading the model...\")\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    \"./distilled_pruned_gpt2_telemetry\",  # Path to the fine-tuned model\n",
    "    quantize_config=quantize_config  # Apply quantization settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967f3626-0e0a-4fae-a439-77f3d3692306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Preparing example sentences for quantization...\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prepare Example Sentences (DO NOT Tokenize Here)\n",
    "print(\"üîπ Preparing example sentences for quantization...\")\n",
    "example_sentences = [\n",
    "    \"Hello, how is the temperature?\",\n",
    "    \"What is the laser bias ?\",\n",
    "    \"Show the channels of the optical modules \",\n",
    "    \"What is the Rx and Tx temperatures ?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b40ca48-6979-41aa-aa6f-3a5c98bb989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Tokenizing and formatting input for quantization...\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Tokenize Sentences and Convert Format \n",
    "print(\"üîπ Tokenizing and formatting input for quantization...\")\n",
    "\n",
    "# Tokenize sentences and extract `input_ids` and `attention_mask`\n",
    "tokenized_examples = tokenizer(\n",
    "    example_sentences,\n",
    "    padding=True,  # Ensure padding\n",
    "    truncation=True,  # Truncate long inputs\n",
    "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Convert tokenized output into a list of dicts (Required Format)\n",
    "examples_for_quantization = [\n",
    "    {\"input_ids\": input_id, \"attention_mask\": attention_mask}\n",
    "    for input_id, attention_mask in zip(tokenized_examples[\"input_ids\"], tokenized_examples[\"attention_mask\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "134a2768-aef7-4400-ae80-a26e714d648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Quantizing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/6\n",
      "INFO - Quantizing attn.c_attn in layer 1/6...\n",
      "INFO - Quantizing attn.c_proj in layer 1/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 1/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 1/6...\n",
      "INFO - Start quantizing layer 2/6\n",
      "INFO - Quantizing attn.c_attn in layer 2/6...\n",
      "INFO - Quantizing attn.c_proj in layer 2/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 2/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 2/6...\n",
      "INFO - Start quantizing layer 3/6\n",
      "INFO - Quantizing attn.c_attn in layer 3/6...\n",
      "INFO - Quantizing attn.c_proj in layer 3/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 3/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 3/6...\n",
      "INFO - Start quantizing layer 4/6\n",
      "INFO - Quantizing attn.c_attn in layer 4/6...\n",
      "INFO - Quantizing attn.c_proj in layer 4/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 4/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 4/6...\n",
      "INFO - Start quantizing layer 5/6\n",
      "INFO - Quantizing attn.c_attn in layer 5/6...\n",
      "INFO - Quantizing attn.c_proj in layer 5/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 5/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 5/6...\n",
      "INFO - Start quantizing layer 6/6\n",
      "INFO - Quantizing attn.c_attn in layer 6/6...\n",
      "INFO - Quantizing attn.c_proj in layer 6/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 6/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 6/6...\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Quantize the Model Using Tokenized Examples\n",
    "print(\"üîπ Quantizing the model...\")\n",
    "model.quantize(examples_for_quantization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd945efe-ab10-4c03-85b6-c1d970f71117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model successfully quantized to 4-bit and running on cuda!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Move Model to CPU (or GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Model successfully quantized to 4-bit and running on {device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc10af42-f169-414c-9861-3f0f9d6fba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Saving the quantized model to ./quantized_distilled_pruned_gpt2_telemetry...\n",
      "‚úÖ Quantized model and tokenizer saved successfully in ./quantized_distilled_pruned_gpt2_telemetry!\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Save the Quantized Model Locally\n",
    "save_directory = \"./quantized_distilled_pruned_gpt2_telemetry\"\n",
    "print(f\"üîπ Saving the quantized model to {save_directory}...\")\n",
    "model.save_quantized(save_directory)  \n",
    "tokenizer.save_pretrained(save_directory)  # Save tokenizer\n",
    "print(f\"‚úÖ Quantized model and tokenizer saved successfully in {save_directory}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e5803b-b8b4-4241-b8fe-b3b7cd2fee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Saved files:\n",
      "['config.json', 'gptq_model-4bit-128g.safetensors', 'merges.txt', 'quantize_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Verify Saved Files\n",
    "import os\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "print(os.listdir(save_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "183a3972-cbd3-4b0e-b8fe-ae4296ed2874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Reloading the quantized model for verification...\n",
      "‚úÖ Quantized model reloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Reload the Saved Quantized Model\n",
    "print(\"\\nüîπ Reloading the quantized model for verification...\")\n",
    "reloaded_model = AutoGPTQForCausalLM.from_quantized(save_directory)\n",
    "reloaded_model.to(device)\n",
    "print(\"‚úÖ Quantized model reloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dfcd44-d577-4640-b091-6aa128273742",
   "metadata": {},
   "source": [
    "# Inference Testing: \n",
    "## Quantized LLM Application: AI Operations (AiOPS) Assistant for the NetAdmin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e67e60-08ca-445f-b23b-44b891becca4",
   "metadata": {},
   "source": [
    "### AiOPS persona: The model acts as an AI-powered network operations assistant.\n",
    "### NetAdmin user role: The user provides network-related queries (e.g., troubleshooting, diagnostics).\n",
    "### Prompt Engineering: Uses a structured input format so the model understands the networking context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ebdf40-7a57-40bf-820f-c0cbdf4b1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b98a4641-f466-409b-b7a2-08e6cb0180f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading AiOPS Assistant for network troubleshooting...\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "save_directory = \"./quantized_distilled_pruned_gpt2_telemetry\"\n",
    "print(\"üîπ Loading AiOPS Assistant for network troubleshooting...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ae80e3-942c-4359-849b-6e2560359cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AiOPS agent is online and running on cuda!\n"
     ]
    }
   ],
   "source": [
    "# Load the quantized model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoGPTQForCausalLM.from_quantized(save_directory)\n",
    "model.to(device)\n",
    "print(f\"‚úÖ AiOPS agent is online and running on {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15ba91-8f6e-4448-8645-9bcee7a81e65",
   "metadata": {},
   "source": [
    "## AiOPS Network Assistant  (Runs on the cell site backhaul router in the cell tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15984ce4-e6c8-4a92-bf4e-576693566763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ NetAdmin, type your network issue. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üëâ Describe the problem (or type 'exit' to quit):  Optical Module not working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è AiOPS Assistant Response:\n",
      "[AiOPS Assistant] NetAdmin request: Optical Module not working\n",
      "[AiOPS Response]: Anomalous - Channel 1 Input Power Over-Coupling Timestamp 0.21 dBm (SFP)\n",
      "response: Normal, QPI+2 Output Voltage 2.19V\n",
      "Response Time 3.62dBM (-1.74 dB m¬≤), Max B\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üëâ Describe the problem (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîª Exiting AiOPS Assistant. Have a great day, NetAdmin! üöÄ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# **Function for AiOPS Network Assistant**\n",
    "def aiops_assist(netadmin_input):\n",
    "    \"\"\"\n",
    "    AiOPS Network Assistant: Analyzes network issues, suggests optimizations, and provides troubleshooting tips.\n",
    "\n",
    "    Args:\n",
    "    - netadmin_input (str): The network-related query or issue from the NetAdmin.\n",
    "\n",
    "    Returns:\n",
    "    - str: AiOPS response with troubleshooting steps or recommendations.\n",
    "    \"\"\"\n",
    "    aiops_prompt = f\"[AiOPS Assistant] NetAdmin request: {netadmin_input}\\n[AiOPS Response]:\"\n",
    "    \n",
    "    inputs = tokenizer(aiops_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=len(netadmin_input) * 3,  # Dynamically adjust response length\n",
    "            do_sample=True,  # Enable diverse response generation\n",
    "            temperature=0.7,  # Control randomness\n",
    "            top_p=0.9,  # Nucleus sampling\n",
    "            repetition_penalty=1.2,  # Reduce repetition\n",
    "            eos_token_id=tokenizer.eos_token_id  # Stop generation when reaching EOS token\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# **Loop for Continuous Interaction**\n",
    "print(\"\\nüí¨ NetAdmin, type your network issue. Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    netadmin_input = input(\"üëâ Describe the problem (or type 'exit' to quit): \")\n",
    "    \n",
    "    # **Exit condition**\n",
    "    if netadmin_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print(\"\\nüîª Exiting AiOPS Assistant. Have a great day, NetAdmin! üöÄ\")\n",
    "        break  # Stop the loop\n",
    "    \n",
    "    # Generate and Display AI-powered Troubleshooting Response\n",
    "    aiops_response = aiops_assist(netadmin_input)\n",
    "    print(\"\\nüõ†Ô∏è AiOPS Assistant Response:\")\n",
    "    print(aiops_response)\n",
    "    print(\"\\n\" + \"-\" * 80)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64873140-3cdd-4d94-8753-9f9e6bff746c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
