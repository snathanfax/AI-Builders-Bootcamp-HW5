{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410d8e03",
   "metadata": {},
   "source": [
    "# Telemetry Pipeline - Full Workflow\n",
    "This notebook generates synthetic switch telemetry data, prepares it for GPT-2 fine-tuning using Hugging Face, performs model compression using  quantization, pruning, and knowledge distillation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae273dc-364f-4692-972e-ceae6d12ffba",
   "metadata": {},
   "source": [
    "                    GNU AFFERO GENERAL PUBLIC LICENSE\n",
    "                       Version 3, 19 November 2007\n",
    "\n",
    "Copyright (C) 2025 Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify  \n",
    "it under the terms of the GNU Affero General Public License as  \n",
    "published by the Free Software Foundation, either version 3 of the  \n",
    "License, or (at your option) any later version.  \n",
    "\n",
    "This program is distributed in the hope that it will be useful,  \n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of  \n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the  \n",
    "GNU Affero General Public License for more details.  \n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License  \n",
    "along with this program. If not, see <https://www.gnu.org/licenses/>.  \n",
    "\n",
    "As per AGPLv3, if you modify this software and make it available over a  \n",
    "network, you must provide the source code of your modifications under the  \n",
    "same license.  \n",
    "\n",
    "For inquiries, please contact:  \n",
    "Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e63565-5723-4695-b305-845f3ad2bc78",
   "metadata": {},
   "source": [
    "# Safe Fine-Tuning and Model Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da416019-0a19-4797-adb8-ca8077421847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Debug GPU Information ---\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Force clearer error reporting from CUDA\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset = load_dataset('json', data_files={'train': 'train.jsonl'})\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure tokenizer has padding token (GPT-2 does not have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load Model Safely ---\n",
    "# Set dtype explicitly to match intended precision (can be float16 if using fp16 training)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "\n",
    "# Move to GPU after verifying load works\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# --- Tokenization Helper ---\n",
    "def concatenate_prompt_response(examples):\n",
    "    combined = [\n",
    "        f\"prompt: {p}\\nresponse: {r}\" for p, r in zip(examples['prompt'], examples['response'])\n",
    "    ]\n",
    "    return tokenizer(combined, truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(concatenate_prompt_response, batched=True)\n",
    "tokenized_eval = eval_dataset.map(concatenate_prompt_response, batched=True)\n",
    "\n",
    "# --- Data Collator (dynamic padding) ---\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# --- Trainer Setup ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --- Train Model ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Safe Save (CPU-based) ---\n",
    "print(\"✅ Training complete. Saving model to CPU...\")\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "model.save_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "\n",
    "print(\"✅ Model and tokenizer saved safely to 'fine_tuned_gpt2_telemetry'.\")\n",
    "\n",
    "# --- Post-save Reload Test ---\n",
    "print(\"✅ Reloading saved model for sanity check...\")\n",
    "\n",
    "reloaded_model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_gpt2_telemetry\", torch_dtype=torch.float32)\n",
    "reloaded_model = reloaded_model.to(\"cuda\")  # Move back to GPU\n",
    "\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "\n",
    "# Quick inference test to confirm save/load worked\n",
    "test_input = \"prompt: What is knowledge distillation?\\nresponse:\"\n",
    "inputs = reloaded_tokenizer(test_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model(**inputs)\n",
    "\n",
    "print(f\"✅ Reloaded model test passed. Output shape: {outputs.logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdeaea9-cbe4-4a76-b315-7ca0e3c9be6f",
   "metadata": {},
   "source": [
    "# Testing for Model Corruption etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4021f0-e772-42f1-8bf5-9d6d22a22cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: Quadro M1000M\n",
      "CUDA Version: 11.7\n",
      "PyTorch Version: 2.0.0+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully to CPU.\n",
      "✅ Model moved to GPU successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt2_telemetry\", device_map=None)\n",
    "    print(\"✅ Model loaded successfully to CPU.\")\n",
    "\n",
    "    teacher = teacher.to(\"cuda\")\n",
    "    print(\"✅ Model moved to GPU successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during model load/move: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b32f07-f629-442e-bbc7-e27438297302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
