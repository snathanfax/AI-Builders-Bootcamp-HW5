{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410d8e03",
   "metadata": {},
   "source": [
    "# Telemetry Pipeline - Model Compression Workflow\n",
    "This notebook takes the fine_tuned_gpt2_telemetry model, performs model compression using knowledge distillation, pruning and quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df3dcd-8a9e-4cbf-9a97-f13fc2a904fe",
   "metadata": {},
   "source": [
    "                    GNU AFFERO GENERAL PUBLIC LICENSE\n",
    "                       Version 3, 19 November 2007\n",
    "\n",
    "Copyright (C) 2025 Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify  \n",
    "it under the terms of the GNU Affero General Public License as  \n",
    "published by the Free Software Foundation, either version 3 of the  \n",
    "License, or (at your option) any later version.  \n",
    "\n",
    "This program is distributed in the hope that it will be useful,  \n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of  \n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the  \n",
    "GNU Affero General Public License for more details.  \n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License  \n",
    "along with this program. If not, see <https://www.gnu.org/licenses/>.  \n",
    "\n",
    "As per AGPLv3, if you modify this software and make it available over a  \n",
    "network, you must provide the source code of your modifications under the  \n",
    "same license.  \n",
    "\n",
    "For inquiries, please contact:  \n",
    "Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ba4b5-caa6-4c5f-97a8-a7658c6cd7d5",
   "metadata": {},
   "source": [
    "## Load Fine Tuned Model for Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4021f0-e772-42f1-8bf5-9d6d22a22cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: Quadro M1000M\n",
      "CUDA Version: 11.7\n",
      "PyTorch Version: 2.0.0+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully to CPU.\n",
      "‚úÖ Model moved to GPU successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt2_telemetry\", device_map=None)\n",
    "    print(\"‚úÖ Model loaded successfully to CPU.\")\n",
    "\n",
    "    teacher = teacher.to(\"cuda\")\n",
    "    print(\"‚úÖ Model moved to GPU successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during model load/move: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f364f3-a2ac-4e5e-b54c-5c8a2efcfd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b153ca808604cf0a47316618304917a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71cedaae3304ecaae0eb746aff29709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\transformers\\training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\shaji.nathan\\AppData\\Local\\Temp\\ipykernel_8464\\3197698090.py:121: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = DistillationTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 4:28:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>33.666700</td>\n",
       "      <td>26.494558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>26.089500</td>\n",
       "      <td>19.545992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>23.715200</td>\n",
       "      <td>19.037495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CPU-only Distillation complete ‚Äî Student saved to 'distilled_gpt2_telemetry'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Load Tokenizer (from fine-tuned teacher) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_gpt2_telemetry\")\n",
    "\n",
    "# GPT-2 does not have a native padding token, so ensure we set one\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load Teacher and Student (Both on CPU to avoid GPU issues) ---\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\"fine_tuned_gpt2_telemetry\", device_map=None).to(\"cpu\")\n",
    "student = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Ensure the student has the same vocabulary size as the teacher\n",
    "student.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# --- Load Dataset ---\n",
    "dataset = load_dataset('json', data_files={'train': 'train.jsonl'})\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# --- Tokenization + Labels Preparation ---\n",
    "# GPT-2 uses causal language modeling, so \"labels\" = \"input_ids\"\n",
    "def process_examples(examples):\n",
    "    combined = [\n",
    "        f\"prompt: {p}\\nresponse: {r}\" for p, r in zip(examples['prompt'], examples['response'])\n",
    "    ]\n",
    "    tokenized = tokenizer(combined, truncation=True, max_length=512, padding=\"max_length\")\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Causal LM needs labels = input_ids\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train = train_dataset.map(process_examples, batched=True)\n",
    "tokenized_eval = eval_dataset.map(process_examples, batched=True)\n",
    "\n",
    "# --- Data Collator for Dynamic Padding ---\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=student,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# --- TrainingArguments (force CPU-only via no_cuda=True) ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilled_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  # Small batch size (safer for CPU)\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # Mixed precision is disabled (no GPU)\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"tensorboard\",\n",
    "    no_cuda=True  # Fully disable CUDA (critical fix for your GPU issues)\n",
    ")\n",
    "\n",
    "# --- Correct Distillation Loss (KL Divergence + CrossEntropy) ---\n",
    "# ‚úÖ This is token prediction across 50257 tokens ‚Äî so we need:\n",
    "#    - CrossEntropyLoss (hard target from dataset labels)\n",
    "#    - KLDivLoss (soft target from teacher logits)\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    \"\"\"\n",
    "    KL Divergence loss between soft probabilities of student and teacher.\n",
    "    \"\"\"\n",
    "    student_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    return F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "\n",
    "# --- Custom Trainer for Knowledge Distillation ---\n",
    "class DistillationTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute combined loss:\n",
    "        1. CrossEntropyLoss (hard label matching against ground truth tokens)\n",
    "        2. KLDivLoss (soft label matching against teacher probabilities)\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        student_logits = outputs.logits\n",
    "\n",
    "        # Forward pass through teacher (always on CPU to avoid GPU issues)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Debugging check (should always match)\n",
    "        assert student_logits.shape == teacher_logits.shape, (\n",
    "            f\"Logits shape mismatch! Student: {student_logits.shape}, Teacher: {teacher_logits.shape}\"\n",
    "        )\n",
    "\n",
    "        # CrossEntropyLoss ‚Äî this is the core loss for language modeling\n",
    "        ce_loss = F.cross_entropy(\n",
    "            student_logits.view(-1, student_logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=tokenizer.pad_token_id  # Ignore padding tokens during loss calc\n",
    "        )\n",
    "\n",
    "        # KLDivLoss ‚Äî this is the distillation component (soft target matching)\n",
    "        distill_loss = distillation_loss(student_logits, teacher_logits)\n",
    "\n",
    "        # Final combined loss (50% CE, 50% distillation)\n",
    "        total_loss = 0.5 * ce_loss + 0.5 * distill_loss\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "# --- Instantiate Trainer ---\n",
    "trainer = DistillationTrainer(\n",
    "    model=student,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Save Distilled Student Model ---\n",
    "student.save_pretrained(\"distilled_gpt2_telemetry\")\n",
    "tokenizer.save_pretrained(\"distilled_gpt2_telemetry\")\n",
    "\n",
    "print(\"‚úÖ CPU-only Distillation complete ‚Äî Student saved to 'distilled_gpt2_telemetry'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf8ea6-b8be-4f5d-99b1-58764079832f",
   "metadata": {},
   "source": [
    "## Load Distilled Model into CPU and GPU to check for corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b32f07-f629-442e-bbc7-e27438297302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: Quadro M1000M\n",
      "CUDA Version: 11.7\n",
      "PyTorch Version: 2.0.0+cu117\n",
      "‚úÖ Model loaded successfully to CPU.\n",
      "‚úÖ Model moved to GPU successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\"./distilled_gpt2_telemetry\", device_map=None)\n",
    "    print(\"‚úÖ Model loaded successfully to CPU.\")\n",
    "\n",
    "    teacher = teacher.to(\"cuda\")\n",
    "    print(\"‚úÖ Model moved to GPU successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during model load/move: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88aa103-c647-40ee-9c8d-562e89dd7820",
   "metadata": {},
   "source": [
    "# Model Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0f51aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model pruned and saved as 'distilled_pruned_gpt2_telemetry'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./distilled_gpt2_telemetry\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.3)\n",
    "\n",
    "model.save_pretrained(\"distilled_pruned_gpt2_telemetry\")\n",
    "print(\"‚úÖ Model pruned and saved as 'distilled_pruned_gpt2_telemetry'.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e9fca-c7d9-4f86-b776-6ebdb57f288d",
   "metadata": {},
   "source": [
    "## Check Distilled and Pruned Model for corruption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64fc25b-dc81-4a80-b481-93450262e03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device: Quadro M1000M\n",
      "CUDA Version: 11.7\n",
      "PyTorch Version: 2.0.0+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./distilled_pruned_gpt2_telemetry were not used when initializing GPT2LMHeadModel: ['lm_head.weight_mask']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully to CPU.\n",
      "‚úÖ Model moved to GPU successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\"./distilled_pruned_gpt2_telemetry\", device_map=None)\n",
    "    print(\"‚úÖ Model loaded successfully to CPU.\")\n",
    "\n",
    "    teacher = teacher.to(\"cuda\")\n",
    "    print(\"‚úÖ Model moved to GPU successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during model load/move: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f6024-0477-4951-933b-c30822482436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
