{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410d8e03",
   "metadata": {},
   "source": [
    "# Telemetry Pipeline - Model Compression Workflow\n",
    "This notebook takes the distilled_pruned_gpt2_telemetry model, performs model compression using quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891c495-20d5-4f02-8700-1805dc3e51db",
   "metadata": {},
   "source": [
    "                    GNU AFFERO GENERAL PUBLIC LICENSE\n",
    "                       Version 3, 19 November 2007\n",
    "\n",
    "Copyright (C) 2025 Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n",
    "\n",
    "This program is free software: you can redistribute it and/or modify  \n",
    "it under the terms of the GNU Affero General Public License as  \n",
    "published by the Free Software Foundation, either version 3 of the  \n",
    "License, or (at your option) any later version.  \n",
    "\n",
    "This program is distributed in the hope that it will be useful,  \n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of  \n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the  \n",
    "GNU Affero General Public License for more details.  \n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License  \n",
    "along with this program. If not, see <https://www.gnu.org/licenses/>.  \n",
    "\n",
    "As per AGPLv3, if you modify this software and make it available over a  \n",
    "network, you must provide the source code of your modifications under the  \n",
    "same license.  \n",
    "\n",
    "For inquiries, please contact:  \n",
    "Shaji R. Nathan  \n",
    "IP Infusion Inc.  \n",
    "Email: shaji.nathan@ipinfusion.com  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e9fca-c7d9-4f86-b776-6ebdb57f288d",
   "metadata": {},
   "source": [
    "## Check Distilled and Pruned Model for corruption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06804f-35d8-432f-8888-3ddaefa3eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "torchvision.disable_beta_transforms_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64fc25b-dc81-4a80-b481-93450262e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\"./distilled_pruned_gpt2_telemetry\", device_map=None)\n",
    "    print(\"‚úÖ Model loaded successfully to CPU.\")\n",
    "\n",
    "    teacher = teacher.to(\"cuda\")\n",
    "    print(\"‚úÖ Model moved to GPU successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during model load/move: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f02e9-ad69-4b77-a7c3-33a0f26b5677",
   "metadata": {},
   "source": [
    "# Quantization of the model \n",
    "### Load and Quantize  GPT-2 Model in 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f018ec9d-84fe-4bbc-8ab7-25d17b49a214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Miniconda3\\envs\\sentence-transformers\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading tokenizer...\n",
      "üîπ Defining 4-bit quantization configuration...\n",
      "üîπ Loading the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./distilled_pruned_gpt2_telemetry were not used when initializing GPT2LMHeadModel: ['lm_head.weight_mask']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Preparing example sentences for quantization...\n",
      "üîπ Tokenizing and formatting input for quantization...\n",
      "üîπ Quantizing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/6\n",
      "INFO - Quantizing attn.c_attn in layer 1/6...\n",
      "INFO - Quantizing attn.c_proj in layer 1/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 1/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 1/6...\n",
      "INFO - Start quantizing layer 2/6\n",
      "INFO - Quantizing attn.c_attn in layer 2/6...\n",
      "INFO - Quantizing attn.c_proj in layer 2/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 2/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 2/6...\n",
      "INFO - Start quantizing layer 3/6\n",
      "INFO - Quantizing attn.c_attn in layer 3/6...\n",
      "INFO - Quantizing attn.c_proj in layer 3/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 3/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 3/6...\n",
      "INFO - Start quantizing layer 4/6\n",
      "INFO - Quantizing attn.c_attn in layer 4/6...\n",
      "INFO - Quantizing attn.c_proj in layer 4/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 4/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 4/6...\n",
      "INFO - Start quantizing layer 5/6\n",
      "INFO - Quantizing attn.c_attn in layer 5/6...\n",
      "INFO - Quantizing attn.c_proj in layer 5/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 5/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 5/6...\n",
      "INFO - Start quantizing layer 6/6\n",
      "INFO - Quantizing attn.c_attn in layer 6/6...\n",
      "INFO - Quantizing attn.c_proj in layer 6/6...\n",
      "INFO - Quantizing mlp.c_fc in layer 6/6...\n",
      "INFO - Quantizing mlp.c_proj in layer 6/6...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model successfully quantized to 4-bit and running on cuda!\n",
      "üîπ Saving the quantized model to ./quantized_distilled_pruned_gpt2_telemetry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quantized model and tokenizer saved successfully in ./quantized_distilled_pruned_gpt2_telemetry!\n",
      "\n",
      "üìÅ Saved files:\n",
      "['config.json', 'gptq_model-4bit-128g.safetensors', 'merges.txt', 'quantize_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n",
      "\n",
      "üîπ Reloading the quantized model for verification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - The layer lm_head is not quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quantized model reloaded successfully!\n",
      "\n",
      "üîπ Running inference with the quantized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Generated Text:\n",
      "The future of AI is in question.\n",
      "\n",
      "\n",
      "- Timestamp: 2025-03-07 09:21:21.737018\n",
      "- Module: QSFP-13\n",
      "- Temperature: 66.37¬∞C\n",
      "- Trans\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install auto-gptq transformers optimum torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "# Step 1: Load Tokenizer\n",
    "print(\"üîπ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Load the base tokenizer\n",
    "\n",
    "# Fix: Add padding token (GPT-2 does not have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use the end-of-sequence token as padding\n",
    "\n",
    "# Step 2: Define the 4-bit Quantization Configuration\n",
    "print(\"üîπ Defining 4-bit quantization configuration...\")\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # Use 4-bit quantization\n",
    "    group_size=128,  # Grouped quantization for better efficiency\n",
    "    desc_act=False  # Disable activation-descending quantization\n",
    ")\n",
    "\n",
    "# Step 3: Load the Pruned and Distilled GPT-2 Model\n",
    "print(\"üîπ Loading the model...\")\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    \"./distilled_pruned_gpt2_telemetry\",  # Path to the fine-tuned model\n",
    "    quantize_config=quantize_config  # Apply quantization settings\n",
    ")\n",
    "\n",
    "# Step 4: Prepare Example Sentences (DO NOT Tokenize Here)\n",
    "print(\"üîπ Preparing example sentences for quantization...\")\n",
    "example_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The future of AI is promising.\",\n",
    "    \"Quantization reduces memory usage while maintaining performance.\",\n",
    "    \"GPT-2 can generate human-like text responses.\"\n",
    "]\n",
    "\n",
    "# Step 5: Tokenize Sentences and Convert Format (FIXED)\n",
    "print(\"üîπ Tokenizing and formatting input for quantization...\")\n",
    "\n",
    "# Tokenize sentences and extract `input_ids` and `attention_mask`\n",
    "tokenized_examples = tokenizer(\n",
    "    example_sentences,\n",
    "    padding=True,  # Ensure padding\n",
    "    truncation=True,  # Truncate long inputs\n",
    "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Convert tokenized output into a list of dicts (Required Format)\n",
    "examples_for_quantization = [\n",
    "    {\"input_ids\": input_id, \"attention_mask\": attention_mask}\n",
    "    for input_id, attention_mask in zip(tokenized_examples[\"input_ids\"], tokenized_examples[\"attention_mask\"])\n",
    "]\n",
    "\n",
    "# Step 6: Quantize the Model Using Tokenized Examples (FIXED)\n",
    "print(\"üîπ Quantizing the model...\")\n",
    "model.quantize(examples_for_quantization)  # ‚úÖ Now it will work!\n",
    "\n",
    "# Step 7: Move Model to CPU (or GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"‚úÖ Model successfully quantized to 4-bit and running on {device}!\")\n",
    "\n",
    "# Step 8: Save the Quantized Model Locally\n",
    "save_directory = \"./quantized_distilled_pruned_gpt2_telemetry\"\n",
    "print(f\"üîπ Saving the quantized model to {save_directory}...\")\n",
    "model.save_quantized(save_directory)  # ‚úÖ Now this will work\n",
    "tokenizer.save_pretrained(save_directory)  # Save tokenizer\n",
    "\n",
    "print(f\"‚úÖ Quantized model and tokenizer saved successfully in {save_directory}!\")\n",
    "\n",
    "# Step 9: Verify Saved Files\n",
    "import os\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "print(os.listdir(save_directory))\n",
    "\n",
    "# Step 10: Reload the Saved Quantized Model\n",
    "print(\"\\nüîπ Reloading the quantized model for verification...\")\n",
    "reloaded_model = AutoGPTQForCausalLM.from_quantized(save_directory)\n",
    "reloaded_model.to(device)\n",
    "print(\"‚úÖ Quantized model reloaded successfully!\")\n",
    "\n",
    "# Step 11: Run Inference with the Quantized Model\n",
    "print(\"\\nüîπ Running inference with the quantized model...\")\n",
    "prompt = \"The future of AI is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Ensure padding\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = reloaded_model.generate(**inputs, max_length=50)\n",
    "\n",
    "# Decode and print output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nüìù Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfcea6c-f65f-4631-9c25-211b255046e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
